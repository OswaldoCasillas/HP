# -*- coding: utf-8 -*-
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Palacio de Hierro ‚Äì Runner MANUAL (elige categor√≠a o URL) con Selenium
# - PLP nueva: selectores robustos (tiles/precios/paginaci√≥n)
# - Hist√≥rico .parquet + diffs
# - Alertas por marca (NEW/CHANGES con descuento ‚â• ALERT_MIN_DISC) -> email
# - Genera XLSX (hoja SNAPSHOT)
# Reqs: selenium webdriver-manager pandas xlsxwriter openpyxl
# Env opcional: SAVE_DIR, EMAIL_*, PALACIO_ALERT_BRANDS, ALERT_MIN_DISC
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

import os, re, time, random, smtplib, io
from email.message import EmailMessage
from pathlib import Path
from datetime import datetime, timezone
import pandas as pd

# Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import (
    TimeoutException, StaleElementReferenceException,
    ElementClickInterceptedException
)
from webdriver_manager.chrome import ChromeDriverManager

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
WAIT = 25
SCROLL_STEP = 700
SCROLL_ROUNDS = 2
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0",
]

LIST_SELECTORS = [
    "article.b-product_tile_item",
    "div.b-product_tile[data-component='search/ProductTile']",
    "div.b-product_tile",
    "div.b-product",
    "li.product",
    "div.product-tile",
]

PRICE_BLOCKS = [
    ".b-product_tile-price", ".b-product_price",
    ".b-product_tile .b-product_price", ".product-pricing"
]

_money_clean = re.compile(r"[^\d.,]")
def parse_price(txt):
    if not txt:
        return None
    s = _money_clean.sub("", txt).strip().replace(",", "")
    try:
        return float(s)
    except ValueError:
        return None

def _env_bool(name, default=False):
    v = os.environ.get(name, "")
    if v == "":
        return default
    return v.lower() not in ("0", "false", "no")

def setup_driver(headless=True):
    co = ChromeOptions()
    if headless:
        co.add_argument("--headless=new")
    co.add_argument("--disable-gpu")
    co.add_argument("--window-size=1366,900")
    co.add_argument("--disable-dev-shm-usage")
    co.add_argument("--no-sandbox")
    co.add_argument("--lang=es-MX")
    co.add_argument("--disable-blink-features=AutomationControlled")
    co.add_argument(f"--user-agent={random.choice(UA_LIST)}")
    co.add_argument("--disable-background-timer-throttling")
    co.add_argument("--disable-renderer-backgrounding")
    co.add_argument("--disable-backgrounding-occluded-windows")
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=co)
    driver.set_page_load_timeout(60)
    return driver

def wait_grid(driver):
    css = ", ".join(LIST_SELECTORS)
    WebDriverWait(driver, WAIT).until(lambda d: len(d.find_elements(By.CSS_SELECTOR, css)) > 0)

def current_tiles(driver):
    css = ", ".join(LIST_SELECTORS)
    return driver.find_elements(By.CSS_SELECTOR, css)

def gentle_scroll(driver, rounds=SCROLL_ROUNDS):
    for _ in range(rounds):
        driver.execute_script(f"window.scrollBy(0, {SCROLL_STEP});")
        time.sleep(0.25)
        driver.execute_script("window.scrollBy(0, -200);")
        time.sleep(0.25)

def _robust_click(driver, el):
    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
        time.sleep(0.05)
        el.click()
        return True
    except (ElementClickInterceptedException, StaleElementReferenceException):
        try:
            ActionChains(driver).move_to_element(el).pause(0.05).click().perform()
            return True
        except Exception:
            try:
                driver.execute_script("arguments[0].click();", el)
                return True
            except Exception:
                return False

PAGINATION_NEXT = [
    "a[data-js-pagination-link][aria-label*='Siguiente']",
    ".b-pagination-elements_list .b-next-btn a",
    ".b-pagination-elements_next a",
    "li.b-pagination-elements_list.b-next-btn a",
    "a.b-pagination-elements_number[aria-label*='Siguiente']",
    "a[rel='next']",
]

def click_next_page(driver, next_page_number):
    try:
        sel = f"a[data-js-pagination-link][data-page-number='{next_page_number}']"
        link = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, sel)))
        if _robust_click(driver, link):
            return True
    except TimeoutException:
        pass
    for sel in PAGINATION_NEXT:
        try:
            a = driver.find_element(By.CSS_SELECTOR, sel)
            if a.is_displayed() and a.is_enabled():
                if _robust_click(driver, a):
                    return True
        except Exception:
            continue
    try:
        icon = driver.find_element(By.CSS_SELECTOR, "i.i-arrow-right-after")
        parent_link = icon.find_element(By.XPATH, "./ancestor::a[1]")
        if parent_link and _robust_click(driver, parent_link):
            return True
    except Exception:
        pass
    return False

def wait_page_changed(driver, prev_count, timeout=WAIT):
    start = time.time()
    prev_url = driver.current_url
    css = ", ".join(LIST_SELECTORS)
    while time.time() - start < timeout:
        time.sleep(0.25)
        if driver.current_url != prev_url:
            return True
        try:
            curr = len(driver.find_elements(By.CSS_SELECTOR, css))
            if curr != prev_count and curr > 0:
                return True
        except Exception:
            pass
    return False

def _extract_text(el, selectors):
    for sel in selectors:
        try:
            node = el.find_element(By.CSS_SELECTOR, sel)
            t = (node.text or "").strip()
            if t:
                return t
        except Exception:
            continue
    return None

def _extract_img(el):
    for sel in ["img[data-js-product-image]", "img.b-product_image", "picture img", "img"]:
        try:
            img_el = el.find_element(By.CSS_SELECTOR, sel)
            src = img_el.get_attribute("src") or img_el.get_attribute("data-src")
            if src and src.startswith("http"):
                return src
        except Exception:
            continue
    return None

def parse_tile(el, page_idx):
    href = None
    try:
        a = el.find_element(By.CSS_SELECTOR, "a[href]")
        href = a.get_attribute("href")
    except Exception:
        pass
    pid = None
    if href:
        m = re.search(r"/(\d{5,})", href)
        if m:
            pid = m.group(1)

    name = _extract_text(el, [
        ".b-product_tile-name h4", ".b-product_tile-title",
        ".b-product_tile-name", "h3.b-product_tile-name"
    ])
    brand = _extract_text(el, [".b-product_tile-brand h4", ".b-product_tile-brand"])

    list_p = sale_p = None
    nums = []
    price_block = None
    for sel in PRICE_BLOCKS:
        try:
            price_block = el.find_element(By.CSS_SELECTOR, sel)
            break
        except Exception:
            continue
    if price_block:
        spans = price_block.find_elements(By.CSS_SELECTOR, ".b-product_price-value, [itemprop='price'], span")
        for sp in spans:
            v = parse_price(sp.get_attribute("content") or sp.text)
            if v is not None:
                nums.append(v)
            try:
                # viejo precio (tachado)
                old_anc = sp.find_elements(
                    By.XPATH,
                    "./ancestor::*[contains(@class,'price-old') or contains(@class,'b-product_price-old')]"
                )
                if old_anc and v is not None:
                    list_p = v
                # precio de oferta
                sale_anc = sp.find_elements(
                    By.XPATH,
                    "./ancestor::*[contains(@class,'price-sales') or contains(@class,'b-product_price-sales')]"
                )
                if sale_anc and v is not None:
                    sale_p = v
            except Exception:
                pass
    if (list_p is None or sale_p is None) and len(nums) >= 2:
        mx, mn = max(nums), min(nums)
        if list_p is None: list_p = mx
        if sale_p is None: sale_p = mn
    if list_p is None and sale_p is None and len(nums) == 1:
        list_p = sale_p = nums[0]

    discount = None
    if list_p and sale_p and sale_p < list_p:
        discount = round((1 - (sale_p / list_p)) * 100, 2)

    return {
        "product_id": pid,
        "sku": pid,
        "name": name,
        "brand": brand,
        "price_currency": "MXN",
        "list_price": list_p,
        "sale_price": sale_p,
        "discount_pct": discount,
        "image_url": _extract_img(el),
        "enlace": href,
        "page_idx": page_idx,
        "captured_at": datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds"),
    }

def scrape_category(url, headless=True, max_pages=50):
    driver = setup_driver(headless=headless)
    rows, seen = [], set()
    try:
        print(f"Abriendo: {url}")
        driver.get(url)
        wait_grid(driver)

        page = 1
        while page <= max_pages:
            print(f"‚Äî P√°gina {page} ‚Äî")
            wait_grid(driver)
            gentle_scroll(driver, rounds=SCROLL_ROUNDS)

            tiles = current_tiles(driver)
            print(f"   tiles={len(tiles)}")
            new_here = 0
            for t in tiles:
                try:
                    data = parse_tile(t, page)
                    key = data.get("product_id") or data.get("enlace")
                    if key and key not in seen:
                        seen.add(key)
                        rows.append(data)
                        new_here += 1
                except Exception:
                    continue
            print(f"   nuevos={new_here}")

            prev_cnt = len(tiles)
            moved = click_next_page(driver, page + 1)
            if not moved:
                print("   (No hay m√°s paginaci√≥n visible)")
                break
            if not wait_page_changed(driver, prev_cnt, timeout=WAIT):
                time.sleep(1.0)
            page += 1
            time.sleep(random.uniform(0.3, 0.7))
        return rows
    finally:
        try: driver.quit()
        except Exception: pass

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hist√≥rico + diffs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _normalize_numeric(df: pd.DataFrame):
    for col in ("list_price", "sale_price", "discount_pct"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

def _latest_previous_parquet(out_prefix: str, save_dir: Path) -> Path | None:
    patt = f"{out_prefix}_snapshot_*.parquet"
    cand = sorted(save_dir.glob(patt))
    return cand[-1] if cand else None

def _compute_diffs(prev_df: pd.DataFrame | None, curr_df: pd.DataFrame):
    """
    Devuelve SIEMPRE 4 valores: (key, changes_df, new_df, removed_df)
    - key: 'product_id' si aplica, si no 'enlace'
    """
    key = "product_id" if ("product_id" in curr_df.columns and curr_df["product_id"].notna().any()) else "enlace"

    curr = curr_df.copy()
    if key not in curr.columns:
        curr[key] = curr["enlace"].astype("string")
    curr.set_index(key, inplace=True, drop=False)

    if prev_df is None or prev_df.empty:
        # Primer snapshot: no hay changes ni removed
        return key, pd.DataFrame(columns=curr.columns), curr.copy(), pd.DataFrame(columns=curr.columns)

    prev = prev_df.copy()
    if key not in prev.columns:
        prev[key] = prev["enlace"].astype("string")
    prev.set_index(key, inplace=True, drop=False)

    new_idx = curr.index.difference(prev.index)
    removed_idx = prev.index.difference(curr.index)
    common = curr.index.intersection(prev.index)

    tol = 0.01
    cols_to_check = [c for c in ("list_price", "sale_price", "discount_pct") if c in curr.columns and c in prev.columns]
    changed_idx = []
    for k in common:
        diffs_ok = []
        for c in cols_to_check:
            a, b = curr.at[k, c], prev.at[k, c]
            try:
                if pd.isna(a) and pd.isna(b):
                    eq = True
                elif pd.isna(a) != pd.isna(b):
                    eq = False
                else:
                    eq = abs(float(a) - float(b)) <= tol
            except Exception:
                eq = (a == b)
            diffs_ok.append(eq)
        if not all(diffs_ok):
            changed_idx.append(k)

    changes = curr.loc[changed_idx].copy()
    new = curr.loc[new_idx].copy()
    removed = prev.loc[removed_idx].copy()
    return key, changes, new, removed

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Alertas por marca (email) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _load_alert_brands() -> set[str]:
    raw = os.environ.get("PALACIO_ALERT_BRANDS", "")
    if not raw:
        return set()
    parts = []
    for line in raw.splitlines():
        parts.extend([p.strip() for p in line.split(",")])
    return {p for p in (s.strip() for s in parts) if p}

def _filter_alert_hits_new(new_df: pd.DataFrame, min_disc: float, brands: set[str]) -> pd.DataFrame:
    if new_df is None or new_df.empty:
        return new_df
    df = new_df.copy()
    if "discount_pct" in df.columns:
        df["discount_pct"] = pd.to_numeric(df["discount_pct"], errors="coerce")
        df = df[df["discount_pct"].fillna(0) >= float(min_disc)]
    if "brand" in df.columns and brands:
        df = df[df["brand"].fillna("").isin(brands)]
    return df

def _filter_alert_hits_changes(chg_df: pd.DataFrame, min_disc: float, brands: set[str]) -> pd.DataFrame:
    if chg_df is None or chg_df.empty:
        return chg_df
    df = chg_df.copy()
    if "discount_pct" in df.columns:
        df["discount_pct"] = pd.to_numeric(df["discount_pct"], errors="coerce")
        df = df[df["discount_pct"].fillna(0) >= float(min_disc)]
    if "brand" in df.columns and brands:
        df = df[df["brand"].fillna("").isin(brands)]
    return df

def _send_email(subject: str, body: str, attachments: list[tuple[str, bytes, str]] | None = None):
    host = os.environ.get("EMAIL_HOST", "")
    port = int(os.environ.get("EMAIL_PORT", "587"))
    user = os.environ.get("EMAIL_USER", "")
    pwd  = os.environ.get("EMAIL_PASS", "")
    to   = os.environ.get("EMAIL_TO", "")
    if not (host and port and user and pwd and to):
        if _env_bool("EMAIL_DEBUG", False):
            print("‚ö†Ô∏è EMAIL_* incompletos: no se env√≠a correo.")
        return
    rec = [x.strip() for x in re.split(r"[;,]", to) if x.strip()]
    msg = EmailMessage()
    msg["From"] = user
    msg["To"] = ",".join(rec)
    msg["Subject"] = subject
    msg.set_content(body)
    for (name, data, mime) in (attachments or []):
        mt, st = (mime.split("/", 1) if mime else ("application", "octet-stream"))
        msg.add_attachment(data, maintype=mt, subtype=st, filename=name)
    with smtplib.SMTP(host, port) as s:
        s.ehlo(); s.starttls(); s.ehlo(); s.login(user, pwd)
        s.send_message(msg, from_addr=user, to_addrs=rec)
    print("üìß Alerta enviada:", subject)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ XLSX y Parquet ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def save_xlsx(df: pd.DataFrame, out_prefix: str, save_dir: Path) -> str:
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fname = f"{out_prefix}_snapshot_{stamp}.xlsx"
    fpath = save_dir / fname
    with pd.ExcelWriter(fpath, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="SNAPSHOT")
        wb = writer.book; ws = writer.sheets["SNAPSHOT"]
        money = wb.add_format({"num_format": "#,##0.00"})
        pct   = wb.add_format({'num_format': '0.00"%"'})
        cols = list(df.columns)
        ws.set_column(0, len(cols)-1, 18)
        for c in ("list_price", "sale_price"):
            if c in cols:
                idx = cols.index(c); ws.set_column(idx, idx, 14, money)
        if "discount_pct" in cols:
            idx = cols.index("discount_pct"); ws.set_column(idx, idx, 12, pct)
        ws.autofilter(0, 0, len(df), len(cols)-1); ws.freeze_panes(1, 0)
    print(f"üìÑ Guardado: {fpath}")
    return str(fpath)

def save_parquet(df: pd.DataFrame, out_prefix: str, save_dir: Path) -> str:
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fname = f"{out_prefix}_snapshot_{stamp}.parquet"
    fpath = save_dir / fname
    df.to_parquet(fpath, index=False)
    print(f"üß± Parquet: {fpath}")
    return str(fpath)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Orquestaci√≥n (una sola categor√≠a/URL) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_one(url: str, out_prefix: str, headless: bool, max_pages: int, save_dir: Path):
    save_dir.mkdir(parents=True, exist_ok=True)

    rows = scrape_category(url=url, headless=headless, max_pages=max_pages)
    df = pd.DataFrame(rows)
    _normalize_numeric(df)
    if "product_id" in df.columns: df["product_id"] = df["product_id"].astype("string")
    if "sku" in df.columns: df["sku"] = df["sku"].astype("string")

    prev_pq = _latest_previous_parquet(out_prefix, save_dir)
    prev_df = None
    if prev_pq and prev_pq.exists():
        try:
            prev_df = pd.read_parquet(prev_pq)
            _normalize_numeric(prev_df)
            for d in (df, prev_df):
                if "product_id" in d.columns: d["product_id"] = d["product_id"].astype("string")
                if "sku" in d.columns: d["sku"] = d["sku"].astype("string")
        except Exception as e:
            print(f"‚ö†Ô∏è No pude leer previo {prev_pq.name}: {e}")

    # diffs SIEMPRE 4 valores
    key, changes_df, new_df, removed_df = _compute_diffs(prev_df, df)

    # alertas por marca con descuento
    brands = _load_alert_brands()
    min_disc = float(os.environ.get("ALERT_MIN_DISC", "30"))
    hits_new = _filter_alert_hits_new(new_df, min_disc, brands)
    hits_changes = _filter_alert_hits_changes(changes_df, min_disc, brands)
    if (hits_new is not None and not hits_new.empty) or (hits_changes is not None and not hits_changes.empty):
        atts = []
        if hits_new is not None and not hits_new.empty:
            bio = io.BytesIO(); hits_new.to_excel(bio, index=False)
            atts.append((f"{out_prefix}_ALERT_NEW.xlsx", bio.getvalue(),
                         "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"))
        if hits_changes is not None and not hits_changes.empty:
            bio = io.BytesIO(); hits_changes.to_excel(bio, index=False)
            atts.append((f"{out_prefix}_ALERT_CHANGES.xlsx", bio.getvalue(),
                         "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"))
        body = (f"URL: {url}\nPrefijo: {out_prefix}\nUmbral: {min_disc}%\n"
                f"Marcas: {', '.join(sorted(brands)) or '(todas)'}\n"
                f"NEW: {0 if hits_new is None else len(hits_new)} | "
                f"CHANGES: {0 if hits_changes is None else len(hits_changes)}")
        _send_email(f"[Scraper] Alertas {out_prefix}", body, atts)

    xlsx_path = save_xlsx(df, out_prefix, save_dir)
    pq_path   = save_parquet(df, out_prefix, save_dir)

    big51 = int(df.get("discount_pct", pd.Series(dtype=float)).fillna(0).ge(51).sum())
    print(f"üî¢ Filas: {len(df)} | ‚â•51%: {big51}")
    print(f"√öltimo previo: {prev_pq.name if prev_pq else '‚Äî'}")
    print(f"NEW={0 if new_df is None else len(new_df)} "
          f"CHANGES={0 if changes_df is None else len(changes_df)} "
          f"REMOVED={0 if removed_df is None else len(removed_df)}")
    return df, xlsx_path, pq_path

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI manual ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CATEGORIES = {
    "mujer":  "https://www.elpalaciodehierro.com/mujer/",
    "hombre": "https://www.elpalaciodehierro.com/hombre/",
    "nina":   "https://www.elpalaciodehierro.com/nina/",
    "nino":   "https://www.elpalaciodehierro.com/nino/",
    "hogar":  "https://www.elpalaciodehierro.com/hogar/",
    "belleza":"https://www.elpalaciodehierro.com/belleza/",
    "ofertas":"https://www.elpalaciodehierro.com/outlet/"
}

def prompt_menu():
    print("Elige una categor√≠a o escribe una URL completa:")
    keys = list(CATEGORIES.keys())
    for i, k in enumerate(keys, 1):
        print(f" {i}) {k} ‚Üí {CATEGORIES[k]}")
    raw = input("N√∫mero/URL: ").strip()
    if raw.isdigit():
        ix = int(raw)
        if 1 <= ix <= len(keys):
            return keys[ix-1], CATEGORIES[keys[ix-1]]
        else:
            print("Opci√≥n inv√°lida, usando 'hombre'.")
            return "hombre", CATEGORIES["hombre"]
    if raw.lower().startswith("http"):
        return "custom", raw
    print("Opci√≥n inv√°lida, usando 'hombre'.")
    return "hombre", CATEGORIES["hombre"]

if __name__ == "__main__":
    save_dir = Path(os.environ.get("SAVE_DIR", "/tmp/palacio_out"))

    cat_key, url = prompt_menu()
    headless = _env_bool("HEADLESS", True)
    try:
        max_pages = int(os.environ.get("MAX_PAGES", "50"))
    except Exception:
        max_pages = 50
    out_prefix = os.environ.get("OUT_PREFIX", f"palacio_{cat_key}")

    hp = input(f"Headless? [Y/n] (actual={headless}): ").strip().lower()
    if hp in ("n", "no", "0"): headless = False
    mp = input(f"Max pages? (actual={max_pages}): ").strip()
    if mp.isdigit(): max_pages = int(mp)
    op = input(f"Prefijo salida? (actual={out_prefix}): ").strip()
    if op: out_prefix = op

    print(f"\n>>> RUN <<<\n  cat/url: {url}\n  headless={headless}\n  max_pages={max_pages}\n  prefix={out_prefix}\n  SAVE_DIR={save_dir}\n")
    run_one(url=url, out_prefix=out_prefix, headless=headless, max_pages=max_pages, save_dir=save_dir)
